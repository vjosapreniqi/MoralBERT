{"cells":[{"cell_type":"markdown","metadata":{"id":"w3OAsdTCV32m"},"source":["#Â Predicting Moral Values in Text\n","### This Code offers predicting moral values from the MoralBERT weights deployad in Hugging Face."]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14841,"status":"ok","timestamp":1724337868403,"user":{"displayName":"Vjosa Preniqi","userId":"03254366230098233872"},"user_tz":-60},"id":"E9VkZKG1WD6u","outputId":"915a74a6-7061-4ebf-b4ba-63206b2ee7be"},"outputs":[],"source":["# Libraries:\n","import pandas as pd\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","from huggingface_hub import PyTorchModelHubMixin\n","from transformers import AutoModel, AutoTokenizer\n","import torch.nn.functional as F"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":716,"status":"ok","timestamp":1724337869116,"user":{"displayName":"Vjosa Preniqi","userId":"03254366230098233872"},"user_tz":-60},"id":"eNd3ddD6gCSl"},"outputs":[],"source":["# BERT model and tokenizer:\n","bert_model = AutoModel.from_pretrained(\"bert-base-uncased\")\n","tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1724337869117,"user":{"displayName":"Vjosa Preniqi","userId":"03254366230098233872"},"user_tz":-60},"id":"5B8MknQBWN-B"},"outputs":[],"source":["class MyModel(\n","    nn.Module,\n","    PyTorchModelHubMixin,\n","    # optionally, you can add metadata which gets pushed to the model card\n","    # repo_url=\"your-repo-url\",\n","    pipeline_tag=\"text-classification\",\n","    license=\"mit\",\n","):\n","    def __init__(self, bert_model, moral_label=2):\n","\n","        super(MyModel, self).__init__()\n","        self.bert = bert_model\n","        bert_dim = 768\n","        self.invariant_trans = nn.Linear(768, 768)\n","        self.moral_classification = nn.Sequential(nn.Linear(768,768),\n","                                                      nn.ReLU(),\n","                                                      nn.Linear(768, moral_label))\n","\n","    def forward(self, input_ids, token_type_ids, attention_mask):\n","        pooled_output = self.bert(input_ids,\n","                                token_type_ids = token_type_ids,\n","                                attention_mask = attention_mask).last_hidden_state[:,0,:]\n","\n","\n","        pooled_output = self.invariant_trans(pooled_output)\n","\n","\n","        logits = self.moral_classification(pooled_output)\n","\n","        return logits"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1724337869117,"user":{"displayName":"Vjosa Preniqi","userId":"03254366230098233872"},"user_tz":-60},"id":"j6NWvkBHWV7H"},"outputs":[],"source":["def preprocessing(input_text, tokenizer):\n","    '''\n","    Returns <class transformers.tokenization_utils_base.BatchEncoding> with the following fields:\n","    - input_ids: list of token ids\n","    - token_type_ids: list of token type ids\n","    - attention_mask: list of indices (0,1) specifying which tokens should considered by the model (return_attention_mask = True).\n","    '''\n","    return tokenizer.encode_plus(\n","                        input_text,\n","                        add_special_tokens = True,\n","                        max_length = 150,\n","                        padding = 'max_length',\n","                        return_attention_mask = True,\n","                        return_token_type_ids = True,  # Add this line\n","                        return_tensors = 'pt',\n","                        truncation=True\n","                   )"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":231},"executionInfo":{"elapsed":36898,"status":"ok","timestamp":1724338740460,"user":{"displayName":"Vjosa Preniqi","userId":"03254366230098233872"},"user_tz":-60},"id":"zJ6Tf3frf78a","outputId":"d2d9a466-afb5-4342-81f1-c50495155dea"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sentence</th>\n","      <th>care</th>\n","      <th>harm</th>\n","      <th>fairness</th>\n","      <th>cheating</th>\n","      <th>loyalty</th>\n","      <th>betrayal</th>\n","      <th>authority</th>\n","      <th>subversion</th>\n","      <th>purity</th>\n","      <th>degradation</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>This is good, but I do not understand it!</td>\n","      <td>0.017906</td>\n","      <td>0.011826</td>\n","      <td>0.000711</td>\n","      <td>0.000608</td>\n","      <td>0.001132</td>\n","      <td>0.000901</td>\n","      <td>0.000606</td>\n","      <td>0.002593</td>\n","      <td>0.001033</td>\n","      <td>0.007981</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>I care a lot for my health and well-being</td>\n","      <td>0.913170</td>\n","      <td>0.000821</td>\n","      <td>0.002522</td>\n","      <td>0.000750</td>\n","      <td>0.001102</td>\n","      <td>0.010791</td>\n","      <td>0.001195</td>\n","      <td>0.001110</td>\n","      <td>0.002080</td>\n","      <td>0.008218</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>You have btrayed your country!!</td>\n","      <td>0.091193</td>\n","      <td>0.001212</td>\n","      <td>0.003562</td>\n","      <td>0.423068</td>\n","      <td>0.992904</td>\n","      <td>0.770573</td>\n","      <td>0.002085</td>\n","      <td>0.003547</td>\n","      <td>0.000779</td>\n","      <td>0.008177</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Breaking the law is bad.</td>\n","      <td>0.019675</td>\n","      <td>0.083457</td>\n","      <td>0.000525</td>\n","      <td>0.002635</td>\n","      <td>0.000824</td>\n","      <td>0.003747</td>\n","      <td>0.925532</td>\n","      <td>0.990706</td>\n","      <td>0.001234</td>\n","      <td>0.025948</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                    sentence      care      harm  fairness  \\\n","0  This is good, but I do not understand it!  0.017906  0.011826  0.000711   \n","1  I care a lot for my health and well-being  0.913170  0.000821  0.002522   \n","2            You have btrayed your country!!  0.091193  0.001212  0.003562   \n","3                   Breaking the law is bad.  0.019675  0.083457  0.000525   \n","\n","   cheating   loyalty  betrayal  authority  subversion    purity  degradation  \n","0  0.000608  0.001132  0.000901   0.000606    0.002593  0.001033     0.007981  \n","1  0.000750  0.001102  0.010791   0.001195    0.001110  0.002080     0.008218  \n","2  0.423068  0.992904  0.770573   0.002085    0.003547  0.000779     0.008177  \n","3  0.002635  0.000824  0.003747   0.925532    0.990706  0.001234     0.025948  "]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["# Example list of sentences\n","sentences = [\n","    \"This is good, but I do not understand it!\",\n","    \"I care a lot for my health and well-being\",\n","    \"You have btrayed your country!!\",\n","    \"Breaking the law is bad.\"\n","    # Add more sentences as needed\n","]\n","\n","# the list of Moral (MFT) values\n","mft_values = [\"care\", \"harm\", \"fairness\", \"cheating\", \"loyalty\", \"betrayal\",\n","              \"authority\", \"subversion\", \"purity\", \"degradation\"]\n","\n","\n","# function to load the model, predict the score, and return the second value\n","def get_model_score(sentence, mft):\n","    repo_name = f\"vjosap/moralBERT-predict-{mft}-in-text\"\n","\n","    # loading the model\n","    model = MyModel.from_pretrained(repo_name, bert_model=bert_model)\n","\n","    # preprocessing the text\n","    encodeds = preprocessing(sentence, tokenizer)\n","\n","    # predicting the mft score\n","    output = model(**encodeds)\n","    score = F.softmax(output, dim=1)\n","\n","    # extracting and return the second value from the tensor\n","    mft_value = score[0, 1].item()\n","\n","    return mft_value\n","\n","# initialising a list to accumulate the results\n","results = []\n","\n","# sequential execution of predictions\n","for sentence in sentences:\n","    # dictionary to store scores for the current sentence\n","    sentence_scores = {\"sentence\": sentence}\n","\n","    # iterate through each MFT model and get the score\n","    for mft in mft_values:\n","        sentence_scores[mft] = get_model_score(sentence, mft)\n","\n","    results.append(sentence_scores)\n","\n","results_df = pd.DataFrame(results)\n","\n","# display the final results\n","results_df\n","\n","# save the DataFrame to a CSV file\n","# results_df.to_csv(\"moral_foundation_scores.csv\", index=False)\n"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyMJhgPuusL6tPXerLnm7qTn","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.1"}},"nbformat":4,"nbformat_minor":0}
